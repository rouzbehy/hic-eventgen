#!/usr/bin/env python3

import argparse
import datetime
from itertools import chain, repeat
import logging
import math
import os
import pickle
import signal
import subprocess
import sys
import tempfile

import numpy as np
import h5py
from scipy.interpolate import interp1d
from scipy.ndimage.measurements import center_of_mass
from scipy.ndimage.interpolation import rotate, shift
import freestream
import frzout


def run_cmd(*args):
    """
    Run and log a subprocess.

    """
    cmd = ' '.join(args)
    logging.info('running command: %s', cmd)
    try:
        proc = subprocess.run(
            cmd.split(), check=True,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            universal_newlines=True
        )
        #print(proc.stdout)
    except subprocess.CalledProcessError as e:
        logging.error(
            'command failed with status %d:\n%s',
            e.returncode, e.output.strip('\n')
        )
        raise
    else:
        logging.debug(
            'command completed successfully:\n%s',
            proc.stdout.strip('\n')
        )
        return proc


class Parser(argparse.ArgumentParser):
    """
    ArgumentParser that parses files with 'key = value' lines.

    """
    def __init__(self, *args, fromfile_prefix_chars='@', **kwargs):
        super().__init__(
            *args, fromfile_prefix_chars=fromfile_prefix_chars, **kwargs
        )

    def convert_arg_line_to_args(self, arg_line):
        # split each line on = and prepend prefix chars to first arg so it is
        # parsed as a long option
        args = [i.strip() for i in arg_line.split('=', maxsplit=1)]
        args[0] = 2*self.prefix_chars[0] + args[0]
        return args


parser = Parser(
    usage=''.join('\n  %(prog)s ' + i for i in [
        '[options] <results_file>',
        'checkpoint <checkpoint_file>',
        '-h | --help',
    ]),
    description='''
Run relativistic heavy-ion collision events.

In the first form, run events according to the given options (below) and write
results to binary file <results_file>.

In the second form, run the event saved in <checkpoint_file>, previously
created by using the --checkpoint option and interrupting an event in progress.
''',
    formatter_class=argparse.RawDescriptionHelpFormatter
)


def parse_args_checkpoint():
    """
    Parse command line arguments according to the parser usage info.  Return a
    tuple (args, ic) where `args` is a normal argparse.Namespace and `ic` is
    either None or an np.array of the checkpointed initial condition.

    First, check for the special "checkpoint" form, and if found, load and
    return the args and checkpoint initial condition from the specified file.
    If not, let the parser object handle everything.

    This is a little hacky but it works fine.  Truth is, argparse can't do
    exactly what I want here.  I suppose `docopt` might be a better option, but
    it's not worth the effort to rewrite everything.

    """
    def usage():
        parser.print_usage(sys.stderr)
        sys.exit(2)

    if len(sys.argv) == 1:
        usage()

    if sys.argv[1] == 'checkpoint':
        if len(sys.argv) != 3:
            usage()

        path = sys.argv[2]

        try:
            with open(path, 'rb') as f:
                args, ic = pickle.load(f)
        except Exception as e:
            msg = '{}: {}'.format(type(e).__name__, e)
            if path not in msg:
                msg += ": '{}'".format(path)
            sys.exit(msg)

        # as a simple integrity check, require that the checkpoint file is
        # actually the file specified in the checkpointed args
        if os.path.abspath(path) != args.checkpoint:
            sys.exit(
                "checkpoint file path '{}' does not match saved path '{}'"
                .format(path, args.checkpoint)
            )

        return args, ic

    return parser.parse_args(), None


parser.add_argument(
    'results', type=os.path.abspath,
    help=argparse.SUPPRESS
)
parser.add_argument(
    '--buffering', type=int, default=0, metavar='INT',
    help='results file buffer size in bytes (default: no buffering)'
)
parser.add_argument(
    '--nevents', type=int, metavar='INT',
    help='number of events to run (default: run until interrupted)'
)
parser.add_argument(
    '--avg-ic', default='off', metavar='VAR',
    help='if on, generate 500 IC events in the centrality bin and take average'
)
parser.add_argument(
    '--afterburner', default='off', metavar='VAR',
    help='if on, perform afterburner (important to compare to exp.)'
)
parser.add_argument(
    '--rankvar', metavar='VAR',
    help='environment variable containing process rank'
)
parser.add_argument(
    '--rankfmt', metavar='FMT',
    help='format string for rank integer'
)
parser.add_argument(
    '--tmpdir', type=os.path.abspath, metavar='PATH',
    help='temporary directory (default: {})'.format(tempfile.gettempdir())
)
parser.add_argument(
    '--checkpoint', type=os.path.abspath, metavar='PATH',
    help='checkpoint file [pickle format]'
)
parser.add_argument(
    '--logfile', type=os.path.abspath, metavar='PATH',
    help='log file (default: stdout)'
)
parser.add_argument(
    '--loglevel', choices={'debug', 'info', 'warning', 'error', 'critical'},
    default='info',
    help='log level (default: %(default)s)'
)
parser.add_argument(
    '--nucleon-width', type=float, default=.5, metavar='FLOAT',
    help='trento nucleon width [fm] (default: %(default)s fm)'
)
parser.add_argument(
    '--grid-step', type=float, default=.1, metavar='FLOAT',
    help='finest grid step [fm] (default: %(default)s fm)'
)
parser.add_argument(
    '--trento-args', default='Pb Pb', metavar='ARGS',
    help="arguments passed to trento (default: '%(default)s')"
)
parser.add_argument(
    '--tau-fs', type=float, default=.5, metavar='FLOAT',
    help='free streaming time [fm] (default: %(default)s fm)'
)
parser.add_argument(
    '--hydro-args', default='', metavar='ARGS',
    help='arguments passed to osu-hydro (default: empty)'
)
parser.add_argument(
    '--Tswitch', type=float, default=.150, metavar='FLOAT',
    help='particlization temperature [GeV] (default: %(default).3f GeV)'
)
parser.add_argument(
    '--centrality-def', default="entropy", metavar='ARGS',
    help='The definition of centrality when enable centrality selection: entropy/npart/impact-parameter'
)
parser.add_argument(
    '--centrality-low', type=float, default=0., metavar='FLOAT',
    help='Lower bound of centrality cut'
)
parser.add_argument(
    '--centrality-high', type=float, default=100., metavar='FLOAT',
    help='Higher bound of centrality cut'
)
parser.add_argument(
    '--system-and-sqrts', default="PbPb2760", metavar='ARGS',
    help='Collision system concatenate with sqrts: PbPb2760/PbPb5020/AuAu200'
)
parser.add_argument(
    '--norm', type=float, default=1., metavar='FLOAT',
    help='normalization of trento'
)




class StopEvent(Exception):
    """ Raise to end an event early. """


def run_events(args, results_file, checkpoint_ic=None, homefolder='./'):
    """
    Run events as determined by user input:

        - Read options from `args`, as returned by `parser.parse_args()`.
        - Write results to binary file object `results_file`.
        - If `checkpoint_ic` is given, run only that IC.

    Return True if at least one event completed successfully, otherwise False.

    """
    ##########################################################################
    ##################Data type, format#######################################
    ##########################################################################
    # species (name, ID) for identified particle observables
    species = [
        ('pion', 211),
        ('kaon', 321),
        ('proton', 2212),
        ('Lambda', 3122),
        ('Sigma0', 3212),
        ('Xi', 3312),
        ('Omega', 3334),
    ]
    # fully specify numeric data types, including endianness and size, to
    # ensure consistency across all machines
    float_t = '<f8'
    int_t = '<i8'
    complex_t = '<c16'
    # results "array" (one element)
    # to be overwritten for each event
    results = np.empty((), dtype=[
        ('initial_entropy', float_t),
        ('nsamples', int_t),
        ('dNch_deta', float_t),
        ('dET_deta', float_t),
        ('dN_dy', [(s, float_t) for (s, _) in species]),
        ('mean_pT', [(s, float_t) for (s, _) in species]),
        ('pT_fluct', [('N', int_t), ('sum_pT', float_t), ('sum_pTsq', float_t)]),
        ('flow', [('N', int_t), ('Qn', complex_t, 8)])
    ])

    # set the grid step size proportionally to the nucleon width
    grid_step = args.grid_step
    # the "target" grid max: the grid shall be at least as large as the target
    grid_max_target = 15
    # next two lines set the number of grid cells and actual grid max,
    # which will be >= the target (same algorithm as trento)
    grid_n = math.ceil(2*grid_max_target/grid_step)
    grid_max = .5*grid_n*grid_step
    logging.info(
        'grid step = %.6f fm, n = %d, max = %.6f fm',
        grid_step, grid_n, grid_max
    )

    def save_initial_condition(path):
        # holder for the data
        attrs = {}
        data = {}
        # load attributes and data to the holder
        with h5py.File(path, 'r') as f:
            print(list(f.keys()))
            for name in f['event_0'].attrs:
                attrs[name] = f['event_0'].attrs[name]
            for name in f['event_0'].keys():
                #data[name] = f['event_0'][name].value
                data[name] = f['event_0'][name][()]

        # write Transposed dataset to the file "initial-T.hdf"
        with h5py.File(homefolder+"/initial.hdf5", 'w') as f:
            gp = f.create_group('event_0')
            for name in attrs:
                gp.attrs.create(name, attrs[name])
            for name in data:
                # take transpose here, with the same level of compression as before
                gp.create_dataset(name, data=data[name].T, 
                                  compression="gzip", compression_opts=4)
        # done

    def _initial_conditions(nevents=1, initial_file='initial.hdf', avg='off'):
        """
        Run trento and yield initial condition arrays.
        """
        def average_ic(fname):
            with h5py.File(fname, 'a') as f:
                densityavg = np.zeros_like(f['event_0/matter_density'][()])
                Ncollavg = np.zeros_like(f['event_0/matter_density'][()])
                dxy = f['event_0'].attrs['dxy']
                Neve = len(f.values())
                for eve in f.values():
                    # step1, center the event
                    NL = int(eve.attrs['Nx']/2)
                    density = eve['matter_density'][()]
                    comxy = -np.array(center_of_mass(density))+np.array([NL, NL])
                    density = shift(density, comxy)
                    Ncoll = shift(eve['Ncoll_density'][()], comxy)
                    # step2, rotate the event to align psi2
                    psi2 = eve.attrs['psi2']
                    imag_psi2 = psi2*180./np.pi + (90. if psi2<0 else -90.)
                    densityavg += rotate(density, angle=imag_psi2, reshape=False)
                    Ncollavg += rotate(Ncoll, angle=imag_psi2, reshape=False)    
                # step3 take average
                densityavg /= Neve
                Ncollavg /= Neve
            # rewrite the initial.hdf file with average ic
            with h5py.File(fname, 'w') as f:
                gp = f.create_group('event_0')
                gp.create_dataset('matter_density', data=densityavg)
                gp.create_dataset('Ncoll_density', data=Ncollavg)
                gp.attrs.create('Nx', densityavg.shape[1])
                gp.attrs.create('Ny', densityavg.shape[0])
                gp.attrs.create('dxy', dxy)            

        try:
            os.remove(initial_file)
        except FileNotFoundError:
            pass
        
        logging.info("Use {} to define centrality.".format(args.centrality_def))
        logging.info("{} < centrality(%) < {}".format(args.centrality_low, 
                                                      args.centrality_high))
        centrality_cut_args = {
          'entropy': "--s-min  {} --s-max  {}",
          'npart': "--npart-min  {} --npart-max {}",
          'impact-parameter': "--b-min {} --b-max {}"
        }[args.centrality_def]
 
        def GetCenTable():
            cl, ch, s, b, n = np.loadtxt(
             os.environ['XDG_DATA_HOME']+'/{:s}_centrality.dat'.format(
                                                args.system_and_sqrts ) ).T
            cm = (cl+ch)/2.
            s *= args.norm
            return {
           'entropy': interp1d(cm, s, kind='linear', fill_value='extrapolate'),
           'npart': interp1d(cm, n, kind='linear', fill_value='extrapolate'),
           'impact-parameter': interp1d(cm, b, kind='linear', fill_value='extrapolate')
                   } 
        CenTable = GetCenTable()    
        interp_cuts = {
          'entropy': [CenTable['entropy'](args.centrality_high), 
                      CenTable['entropy'](args.centrality_low) ],
          'npart'  : [int(CenTable['npart'](args.centrality_high)),
                      int(CenTable['npart'](args.centrality_low)) ],
          'impact-parameter': [CenTable['impact-parameter'](args.centrality_low),
                               CenTable['impact-parameter'](args.centrality_high) ]
        }
        cutlow, cuthigh = interp_cuts[args.centrality_def]
        logging.info("--> {} < {} < {}".format(cutlow, args.centrality_def, cuthigh))
        cut_args = centrality_cut_args.format(cutlow, cuthigh)
        
        if args.centrality_def != 'impact-parameter':
            # if the cut is not impact-parameter and the bin is small, the acceptance
            # rate of trento will be too small, to approximately solve this proble,
            # get the impact parameter cut as well, then increase the b-range to allow 
            # fluctuations, then apply both centrality and b-cut to event generation
            blow, bhigh = interp_cuts['impact-parameter']
            blow = np.max([0., blow-2.])
            bhigh = bhigh+2.
            logging.info("with a wider range of b-selection {} < {} < {}".format(blow, 'impact-parameter', bhigh))
            cut_args = cut_args + " --b-min {} --b-max {}".format(blow, bhigh)
        if avg == 'on':
            logging.info("averaged initial condition mode, could take a while")
        run_cmd(
            'trento',
            '--number-events {}'.format(nevents if avg=='off' else 1000),
            '--grid-step {} --grid-max {}'.format(grid_step, grid_max_target),
            '--output', initial_file,
            '--nucleon-width {}'.format(args.nucleon_width),
            '--normalization {}'.format(args.norm),
            cut_args,
            args.trento_args,
        )
        if avg == 'on':
            logging.info(f"taking average over 1000 trento events")
            average_ic(initial_file)

        ### create a transpose IC file (to match hydro grid) in the homefolder
        save_initial_condition(initial_file)

        ### create iterable initial conditon generator
        with h5py.File(initial_file, 'r') as f:
            for dset in f.values():
                ic = np.array(dset['matter_density'])
                # Write the checkpoint file _before_ starting the event so that
                # even if the process is forcefully killed, the state will be
                # saved.  If / when all events complete, delete the file.
                if args.checkpoint is not None:
                    with open(args.checkpoint, 'wb') as cf:
                        pickle.dump((args, ic), cf, pickle.HIGHEST_PROTOCOL)
                    logging.info('wrote checkpoint file %s', args.checkpoint)
                yield ic


    if checkpoint_ic is None:
        # if nevents was specified, generate that number of initial conditions
        # otherwise generate indefinitely
        initial_conditions = (
            chain.from_iterable(_initial_conditions() for _ in repeat(None))
            if args.nevents is None else
            _initial_conditions(args.nevents, avg=args.avg_ic)
        )
    else:
        # just run the checkpointed IC
        initial_conditions = [checkpoint_ic]

    # create sampler HRG object (to be reused for all events)
    hrg_kwargs = dict(species='urqmd' if args.afterburner=='on' else 'all', res_width=True)
    hrg = frzout.HRG(args.Tswitch, **hrg_kwargs)

    # append switching energy density to hydro arguments
    eswitch = hrg.energy_density()
    hydro_args = [args.hydro_args, 'edec={}'.format(eswitch)]

    # arguments for "coarse" hydro pre-runs
    # no viscosity, run down to low temperature 110 MeV
    hydro_args_coarse = [
        'etas_hrg=0 etas_min=0 etas_slope=0 zetas_max=0 zetas_width=0',
        'edec={}'.format(frzout.HRG(.110, **hrg_kwargs).energy_density())
    ]

    def save_fs_with_hydro(ic):
        # roll ic by index 1 to match hydro
        #ic = np.roll(np.roll(ic, shift=-1, axis=0), shift=-1, axis=1)
        # use same grid settings as hydro output
        with h5py.File('JetData.h5','a') as f:
            taufs = f['Event'].attrs['Tau0'][0]
            dtau = f['Event'].attrs['dTau'][0]
            dxy = f['Event'].attrs['DX'][0]
            ls = f['Event'].attrs['XH'][0]
            n = 2*ls + 1
            coarse = int(dxy/grid_step+.5)
            # [tau0, tau0+dtau, tau0+2*dtau, ..., taufs - dtau] + hydro steps...
            nsteps = int(taufs/dtau)
            tau0 = taufs-dtau*nsteps
            if tau0 < 1e-2: # if tau0 too small, skip the first step
                tau0 += dtau
                nsteps -= 1
            taus = np.linspace(tau0, taufs-dtau, nsteps)
            for tau in taus:
                logging.info(f"tau {tau}")
            # First, rename hydro frames and leave the first few name slots to FS
            event_gp = f['Event']
            for i in range(len(event_gp.keys()))[::-1]:
                old_name = 'Frame_{:04d}'.format(i)
                new_name = 'Frame_{:04d}'.format(i+nsteps)
                event_gp.move(old_name, new_name)
            # Second, overwrite tau0 with FS starting time, and save taufs where
            # FS and hydro is separated
            event_gp.attrs.create('Tau0', [tau0])
            event_gp.attrs.create('TauFS', [taufs])
            # Thrid, fill the first fill steps with FS results
            for itau, tau in enumerate(taus):
                logging.info(f"Free Streaming itau:{itau} --> tau:{tau}")
                frame = event_gp.create_group('Frame_{:04d}'.format(itau))
                fs = freestream.FreeStreamer(ic, grid_max, tau)
                for fmt, data, arglist in [
                    ('e', fs.energy_density, [()]),
                    ('V{}', fs.flow_velocity, [(1,), (2,)]),
                    ('Pi{}{}', fs.shear_tensor, [(0,0), (0,1), (0,2),
                                                        (1,1), (1,2),
                                                               (2,2)] ),
                    ]:
                    for a in arglist:
                        X = data(*a).T # to get the correct x-y with vishnew
                        if fmt == 'V{}': # Convert u1, u2 to v1, v2
                            X = X/data(0).T
                        X = X[::coarse, ::coarse]
                        diff = X.shape[0] - n
                        start = int(abs(diff)/2)
                        if diff > 0:
                            # original grid is larger -> cut out middle square
                            s = slice(start, start + n)
                            X = X[s, s]
                        elif diff < 0:
                            # original grid is smaller
                            #  -> create new array and place original grid in middle
                            Xn = np.zeros((n, n))
                            s = slice(start, start + X.shape[0])
                            Xn[s, s] = X
                            X = Xn
                        if fmt == 'V{}':
                            Comp = {1:'x', 2:'y'}
                            frame.create_dataset(fmt.format(Comp[a[0]]), data=X)
                        if fmt == 'e':
                            frame.create_dataset(fmt.format(*a), data=X)
                            frame.create_dataset('P', data=X/3.)
                            frame.create_dataset('BulkPi', data=X*0.)
                            prefactor = 1.0/15.62687/5.068**3 
                            frame.create_dataset('Temp', data=(X*prefactor)**0.25)
                            #s = (X + frame['P'].value)/(frame['Temp'].value+1e-14)
                            s = (X + frame['P'][()])/(frame['Temp'][()]+1e-14)
                            frame.create_dataset('s', data=s)
                        if fmt == 'Pi{}{}': 
                            frame.create_dataset(fmt.format(*a), data=X)
                #pi33 = -(frame['Pi00'].value + frame['Pi11'].value \
                #                             + frame['Pi22'].value)
                pi33 = -(frame['Pi00'][()]+ frame['Pi11'][()] \
                                          + frame['Pi22'][()])
                frame.create_dataset('Pi33', data=pi33)
                pi3Z = np.zeros_like(pi33)
                frame.create_dataset('Pi03', data=pi3Z)
                frame.create_dataset('Pi13', data=pi3Z)
                frame.create_dataset('Pi23', data=pi3Z)

    def run_hydro(ic, event_size, coarse=False, dt_ratio=.25):
        """
        Run the initial condition contained in FreeStreamer object `fs` through
        osu-hydro on a grid with approximate physical size `event_size` [fm].
        Return a dict of freeze-out surface data suitable for passing directly
        to frzout.Surface.

        Initial condition arrays are cropped or padded as necessary.

        If `coarse` is an integer > 1, use only every `coarse`th cell from the
        initial condition arrays (thus increasing the physical grid step size
        by a factor of `coarse`).  Ignore the user input `hydro_args` and
        instead run ideal hydro down to a low temperature.

        `dt_ratio` sets the timestep as a fraction of the spatial step
        (dt = dt_ratio * dxy).  The SHASTA algorithm requires dt_ratio < 1/2.

        """
        # first freestream
        fs = freestream.FreeStreamer(ic, grid_max, args.tau_fs)
        dxy = grid_step * (coarse or 1)
        ls = math.ceil(event_size/dxy)  # the osu-hydro "ls" parameter
        n = 2*ls + 1  # actual number of grid cells
        for fmt, f, arglist in [
                ('ed', fs.energy_density, [()]),
                ('u{}', fs.flow_velocity, [(1,), (2,)]),
                ('pi{}{}', fs.shear_tensor, [(1, 1), (1, 2), (2, 2)]),
        ]:
            for a in arglist:
                X = f(*a)

                if coarse:
                    X = X[::coarse, ::coarse]

                diff = X.shape[0] - n
                start = int(abs(diff)/2)

                if diff > 0:
                    # original grid is larger -> cut out middle square
                    s = slice(start, start + n)
                    X = X[s, s]
                elif diff < 0:
                    # original grid is smaller
                    #  -> create new array and place original grid in middle
                    Xn = np.zeros((n, n))
                    s = slice(start, start + X.shape[0])
                    Xn[s, s] = X
                    X = Xn

                X.tofile(fmt.format(*a) + '.dat')

        dt = dxy*dt_ratio
        run_cmd(
            'osu-hydro',
            't0={} dt={} dxy={} nls={}'.format(args.tau_fs, dt, dxy, ls),
            *(hydro_args_coarse if coarse else hydro_args)
        )
        surface = np.fromfile('surface.dat', dtype='f8').reshape(-1, 26)
        #Hyper surface output: 
        #Columns: 0    1  2  3    4.................7  8........10
        #Names  : tau  x  y  eta  ds_(tau, x, y, eta)  v_(x,y,eta)
        #Columns: 11     12    13    14    15    16    17    18    19    20
        #Names  : pi_00  pi01  pi02  pi03  pi11  pi12  pi13  pi22  pi23  pi33
        #Columns: 21      22  23  24  25
        #Namse  : BulkPi, T   e   P   mu_B

        if not coarse:
            # only cp back info for the actually run, not the coarse run
            subprocess.call('mv ./surface.dat {}'.format(homefolder), shell=True)
            logging.info("Save free streaming history with hydro histroy")
            save_fs_with_hydro(ic)
            subprocess.call('mv ./JetData.h5 {}'.format(homefolder), shell=True)
            
        # end event if the surface is empty -- this occurs in ultra-peripheral
        # events where the initial condition doesn't exceed Tswitch
        if surface.size == 0:
            raise StopEvent('empty surface')

        # pack surface data into a dict suitable for passing to frzout.Surface
        return dict(
            x=surface[:, 0:3],
            sigma=surface[:, 4:7],
            v=surface[:, 8:10],
            pi=dict(xx=surface.T[15],xy=surface.T[16], yy=surface.T[18]),
            Pi=surface.T[21]
        )

    def save_final_particle_list(fname, event_id):
        with open('particles_out.dat', 'rb') as f:
            delim = np.array([0]+[l.split()[4] for l in f if l.startswith(b'#')],
                             dtype=int).cumsum()
        with open('particles_out.dat', 'rb') as f:
            parts = np.fromiter(
                (tuple(l.split()) for l in f if not l.startswith(b'#')),
                dtype=[
                    ('ID', int), ('charge', int), ('pT', float),
                    ('ET', float), ('mT', float), ('phi', float),
                    ('y', float), ('eta', float),
                ]
            )
        with h5py.File(homefolder+'/final_hadrons.hdf', 'a') as fout:
            gpname = 'event_{}'.format(event_id)
            if gpname in fout:
                del fout[gpname]
            gp = fout.create_group(gpname)
            for count, (il, ih) in enumerate(zip(delim[:-1], delim[1:])):
                gpi = gp.create_group('oversample_{}'.format(count))
                gpi.attrs.create('N_particles', ih-il, dtype=int)
                for name, dtype in \
                    zip(['ID', 'charge', 'pT', 'ET', 'mT', 'phi', 'y', 'eta'],
                        ['i8']*2+['f']*6):
                    gpi.create_dataset(name, data=parts[name][il:ih], 
                                       dtype=dtype)

    def calculate_eventplanes(phi):
        EP_freezeout = {}
        Ns = phi.size
        for n in range(1,8):
            qy = np.sin(n*phi) # array of cos(n*phi_i)
            Qy = qy.sum()       # Qx = N<cos(n*phi)>
            qx = np.cos(n*phi) # array of cos(n*phi_i)
            Qx = qx.sum()       # Qy = N<sin(n*phi)>
            cov = np.cov([qy*Ns, qx*Ns])/(Ns-1.) # covariance between Qy and Qx array
            varQy = cov[0,0]; varQx = cov[1,1]; covQxQy = cov[0,1]
            Psin = np.arctan2(Qy, Qx)/n
            Psinerr = 1./n*np.sqrt(varQy*Qx**2 + varQx*Qy**2 - 2.*covQxQy*Qy*Qx) \
                  /(Qy**2+Qx**2)
            Vn = np.sqrt((Qx**2+Qy**2 - Ns)/Ns/(Ns-1.))
            Vnerr = np.sqrt(Qx**2*varQx + Qy**2*varQy + 2.*Qx*Qy*covQxQy)\
                        /Ns/(Ns - 1.)/Vn
            EP_freezeout[n] = {'Vn' : {'mean': Vn, 'err': Vnerr},
                               'Psin': {'mean': Psin, 'err': Psinerr}}
        return EP_freezeout

    ##########################################################################
    #########################RUN A Event######################################
    ##########################################################################
    def run_single_event(ic):
        """
        Run the initial condition event contained in HDF5 dataset object `ic`
        and save observables to `results`.

        """
        results.fill(0)
        results['initial_entropy'] = ic.sum() * grid_step**2

        assert all(n == grid_n for n in ic.shape)

        
        logging.info(
            'free streaming initial condition for %.3f fm',
            args.tau_fs
        )

        # run coarse event on large grid and determine max radius
        rmax = math.sqrt((
            run_hydro(ic, event_size=27, coarse=3)['x'][:, 1:3]**2
        ).sum(axis=1).max())
        logging.info('rmax = %.3f fm', rmax)

        # now run normal event with size set to the max radius
        # and create sampler surface object
        surface = frzout.Surface(**run_hydro(ic, event_size=rmax), ymax=2)
        logging.info('%d freeze-out cells', len(surface))

        # High precision event planes at freezeout
        logging.info('calculating high-percision event planes @ freezeout')
        phi = np.empty(0)
        count = 0
        while phi.size < 10**6:
            if phi.size > count*50000:
                logging.info("{} particles accepted".format(phi.size))
                count += 1
            parts = frzout.sample(surface, hrg)
            abspid = np.abs(parts['ID'])
            is_pion = (abspid == 111) | (abspid == 211)
            px = parts['p'][:,1]
            py = parts['p'][:,2]
            # select only pion with pT > 0.3 GeV
            cut = (px**2 + py**2 > 0.3**2) & is_pion
            phi = np.append(phi, np.arctan2(py[cut], px[cut]))
        EP_freezeout = calculate_eventplanes(phi)
        Ns = phi.size
        del phi # save some space
        with open(homefolder+"/EventPlanesFrzout.dat",'w') as f:
            print("# {} pions (pT>.3, |y|<2.0) are sampled to calculate the event-planes".format(Ns),file=f)
            print("# order\tV_n\tV_n-err\tPsi_n\tPsi_n-err", file=f)
            for n in range(1,8):
                ep = EP_freezeout[n]
                print(n, ep['Vn']['mean'], ep['Vn']['err'], 
                    ep['Psin']['mean'], ep['Psin']['err'], file=f)

        if args.afterburner == 'on':
            # Sampling particle for UrQMD events
            logging.info('sampling surface with frzout')
            minsamples, maxsamples = 10, 1000  # reasonable range for nsamples
            minparts = 10**5  # min number of particles to sample
            nparts = 0  # for tracking total number of sampled particles
            with open('particles_in.dat', 'w') as f:
                for nsamples in range(1, maxsamples + 1):
                    parts = frzout.sample(surface, hrg)
                    if parts.size == 0:
                        continue
                    nparts += parts.size
                    print('#', parts.size, file=f)
                    for p in parts:
                        print(p['ID'], *p['x'], *p['p'], file=f)
                    if nparts >= minparts and nsamples >= minsamples:
                        break
            # try to free some memory
            # (up to ~a few hundred MiB for ultracentral collisions)
            del surface
            results['nsamples'] = nsamples
            logging.info('produced %d particles in %d samples', nparts, nsamples)

            # Stop event if nothing produced
            if nparts == 0:
                raise StopEvent('no particles produced')

            # hadronic afterburner
            run_cmd('afterburner particles_in.dat particles_out.dat')

            # read final particle data
            with open('particles_out.dat', 'rb') as f:
                parts = np.fromiter(
                    (tuple(l.split()) for l in f if not l.startswith(b'#')),
                    dtype=[
                        ('ID', int),
                        ('charge', int),
                        ('pT', float),
                        ('ET', float),
                        ('mT', float),
                        ('phi', float),
                        ('y', float),
                        ('eta', float),
                    ]
                )
            logging.info('computing observables')
            charged = (parts['charge'] != 0)
            abs_eta = np.fabs(parts['eta'])

            results['dNch_deta'] = \
                np.count_nonzero(charged & (abs_eta < .5)) / nsamples

            ET_eta = .6
            results['dET_deta'] = \
                parts['ET'][abs_eta < ET_eta].sum() / (2*ET_eta) / nsamples

            abs_ID = np.abs(parts['ID'])
            midrapidity = (np.fabs(parts['y']) < .5)
            pT = parts['pT']
            phi = parts['phi']
            for name, i in species:
                cut = (abs_ID == i) & midrapidity
                N = np.count_nonzero(cut)
                results['dN_dy'][name] = N / nsamples
                results['mean_pT'][name] = (0. if N == 0 else pT[cut].mean())

            pT_alice = pT[charged & (abs_eta < .8) & (.15 < pT) & (pT < 2.)]
            results['pT_fluct']['N'] = pT_alice.size
            results['pT_fluct']['sum_pT'] = pT_alice.sum()
            results['pT_fluct']['sum_pTsq'] = np.inner(pT_alice, pT_alice)

            phi_alice = phi[charged & (abs_eta < 1.) & (.15 < pT) & (pT < 2.)]
            results['flow']['N'] = phi_alice.size
            results['flow']['Qn'] = [
                np.exp(1j*n*phi_alice).sum()
                for n in range(1, results.dtype['flow']['Qn'].shape[0] + 1)
            ]
        else:
            logging.info("Event without hadronic afterburner")
            logging.info("Afterburner is important if to be compared to exp!")

    nfail = 0

    # run each initial condition event and save results to file
    for n, ic in enumerate(initial_conditions, start=1):
        logging.info('starting event %d', n)

        try:
            run_single_event(ic)
            # save a copy of oversampled final particle list 
            # its numbering still starts from 0
            if args.afterburner == 'on':
                save_final_particle_list('particles_out.dat', n-1)

        except StopEvent as e:
            logging.info('event stopped: %s', e)
        except Exception:
            logging.exception('event %d failed', n)
            nfail += 1
            if nfail > 3 and nfail/n > .5:
                logging.critical('too many failures, stopping events')
                break
            logging.warning('continuing to next event')
            continue

        results_file.write(results.tobytes())
        logging.info('event %d completed successfully', n)

    # end of events: if running with a checkpoint, delete the file unless this
    # was a failed re-run of a checkpoint event
    if args.checkpoint is not None:
        if checkpoint_ic is not None and nfail > 0:
            logging.info(
                'checkpoint event failed, keeping file %s',
                args.checkpoint
            )
        else:
            os.remove(args.checkpoint)
            logging.info('removed checkpoint file %s', args.checkpoint)

    return n > nfail


def main():
    args, checkpoint_ic = parse_args_checkpoint()

    if checkpoint_ic is None:
        # starting fresh -> truncate output files
        filemode = 'w'

        # must handle rank first since it affects paths
        if args.rankvar:
            rank = os.getenv(args.rankvar)
            if rank is None:
                sys.exit('rank variable {} is not set'.format(args.rankvar))

            if args.rankfmt:
                rank = args.rankfmt.format(int(rank))

            # append rank to path arguments, e.g.:
            #   /path/to/output.log  ->  /path/to/output/<rank>.log
            for a in ['results', 'logfile', 'checkpoint']:
                value = getattr(args, a)
                if value is not None:
                    root, ext = os.path.splitext(value)
                    setattr(args, a, os.path.join(root, rank) + ext)
    else:
        # running checkpoint event -> append to existing files
        filemode = 'a'

    os.makedirs(os.path.dirname(args.results), exist_ok=True)

    if args.logfile is None:
        logfile_kwargs = dict(stream=sys.stdout)
    else:
        logfile_kwargs = dict(filename=args.logfile, filemode=filemode)
        os.makedirs(os.path.dirname(args.logfile), exist_ok=True)

    if args.checkpoint is not None:
        os.makedirs(os.path.dirname(args.checkpoint), exist_ok=True)

    logging.basicConfig(
        level=getattr(logging, args.loglevel.upper()),
        format='[%(levelname)s@%(relativeCreated)d] %(message)s',
        **logfile_kwargs
    )
    logging.captureWarnings(True)

    start = datetime.datetime.now()
    if checkpoint_ic is None:
        logging.info('started at %s', start)
        logging.info('arguments: %r', args)
    else:
        logging.info(
            'restarting from checkpoint file %s at %s',
            args.checkpoint, start
        )

    # translate SIGTERM to KeyboardInterrupt
    signal.signal(signal.SIGTERM, signal.default_int_handler)
    logging.debug('set SIGTERM handler')
    
    with \
            open(args.results, filemode + 'b',
                 buffering=args.buffering) as results_file, \
            tempfile.TemporaryDirectory(
                prefix='hic-', dir=args.tmpdir) as workdir:
        homefolder=os.getcwd()
        os.chdir(workdir)
        logging.info('working directory: %s', workdir)

        try:
            status = run_events(args, results_file, checkpoint_ic, homefolder)
        except KeyboardInterrupt:
            # after catching the initial SIGTERM or interrupt, ignore them
            # during shutdown -- this ensures everything will exit gracefully
            # in case of additional signals (short of SIGKILL)
            signal.signal(signal.SIGTERM, signal.SIG_IGN)
            signal.signal(signal.SIGINT, signal.SIG_IGN)
            status = True
            logging.info(
                'interrupt or signal at %s, cleaning up...',
                datetime.datetime.now()
            )
            if args.checkpoint is not None:
                logging.info(
                    'current event saved in checkpoint file %s',
                    args.checkpoint
                )

    end = datetime.datetime.now()
    logging.info('finished at %s, %s elapsed', end, end - start)

    if not status:
        sys.exit(1)


if __name__ == "__main__":
    main()
